!! Google Code Jam - Predicting Code Efficiency Automatically

!!! Problem and Tasks

Google Code Jam is one of the largest online coding competition with over 10,000 participants every year. The competition consists of several rounds. In each round, participants need to solve a few tasks, each of which will be tested on two sets of inputs, a small one and a large one. The small input set can often be solved using brute force algorithms. For the big test set a more sophisticated implementation is required.

By learning the efficiency on the two inputs, we want to study whether it is possible to automatically predict the runtime of a given implementation, by only looking at static high-level features of the code, so without actually executing it. Related work has been done to detect source code plagiarism and outsourcing, like in [1], and probabilistic graphical models have been applied to related topics, like static analysis of source code in [2].

We will focus on the question of predicting runtime performance of code, using only the submissions in C++, as this is by far the most popular language in the contest. After learning separate classifiers for single tasks, we want to study how we can train a classifier for multiple tasks at once.

Such a classifier can help organizers of coding contests to detect outliers and new types of solutions. A participant that tries to write a fast program for a task might also benefit from our classifier, as it can suggest possible reasons why the student's code is slow.

!!! Algorithms

We will use different models we learned in class, like Naive Bayes and logistic regression model, to train classifers for single tasks. For multi-task learning (MTL) we plan to implement the hierarchical approach for logistic regression, described in [3]. We might also consider other MTL approaches like [4] and [5].

A large part of our project will consist of preparing the data set, as we need to extract features from the source codes and we want to compile and run them to measure their runtime.

!!! Resources

We will crawl these two sites [[http://www.go-hero.net/jam/13/ | Code Jam Language Stats]] and [[https://code.google.com/codejam | Google Code Jam]] to get the source code for different tasks and we will write tools to extract features like user information, code length, number of variables, number of nested loops, libraries used and so on.

!!! Challenges

It is hard to find the set of features to represent the source code properties which are relevant to predict its efficiency. Although we would try different models on our problem, we do not know whether they will be expressive enough to successfully distinguish between slow and fast implementations. Finally, writing a solver for a hierarchical logistic regression model might also be a challenge.

!!! References
 (:linebreaks:)
[1] Elenbogen, Bruce S., and Naeem Seliya.
[[http://dl.acm.org/ft_gateway.cfm?id=1295123&ftid=466864&dwn=1&CFID=324710951&CFTOKEN=75941419 | "Detecting outsourced student programming assignments." ]] 
Journal of Computing Sciences in Colleges 23.3 (2008): 50-57.

[2] Kremenek, Ted, et al.
[[http://www.usenix.org/events/osdi06/tech/full_papers/kremenek/kremenek.pdf|"From uncertainty to belief: Inferring the specification within."]]
Proceedings of the 7th symposium on Operating systems design and implementation. USENIX Association, 2006.

[3] Lapedriza, Àgata, David Masip, and Jordi Vitrià.
[[http://link.springer.com/content/pdf/10.1007%2F978-3-540-72849-8_33.pdf|"A hierarchical approach for multi-task logistic regression."]]
Pattern Recognition and Image Analysis. Springer Berlin Heidelberg, 2007. 258-265.

[4] Obozinski, Guillaume, Ben Taskar, and Michael I. Jordan.
[[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.94.951&rep=rep1&type=pdf|"Multi-task feature selection."]]
Statistics Department, UC Berkeley, Tech. Rep (2006).

[5] Xue, Ya, et al.
[[http://people.ee.duke.edu/~xjliao/paper/JMLR07multiTaskDP.pdf|"Multi-task learning for classification with Dirichlet process priors."]]
The Journal of Machine Learning Research 8 (2007): 35-63.

[[Profiles.dgraf]]
[[Profiles.jjyan]]